---
layout: post
title: "深度学习"
description: ""
categories: []
tags: []
redirect_from:
  - /2020/03/11/
---

* Karmdown table of content
{:toc .toc}


#梯度问题

## sigmoid

缺点：
1. sigmoid更容易引起梯度消失的问题。原因有两个：sigmoid导数小于0.25，反向传播过程中容易导致梯度相乘结果趋于0；sigmoid在x>5的时候导数已经趋于0了。
2. sigmoid函数输出不是“零为中心”(zero-centered）。一个多层的sigmoid神经网络，如果你的输入x都是正数，那么在反向传播中w的梯度传播到网络的某一处时，权值的变化是要么全正要么全负。
3. 指数函数的计算是比较消耗计算资源的。

## tanh

优点：
解决了sigmoid的输出非“零为中心”的问题。

缺点：
1. 依然有sigmoid函数过饱和的问题。
2. 依然指数运算。

## relu

优点：
1. ReLU解决了梯度消失的问题，至少x在正区间内，神经元不会饱和。
2. 由于ReLU线性、非饱和的形式，在SGD中能够快速收敛。
3. 计算速度要快很多。ReLU函数只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快。

缺点：
1. ReLU的输出不是“零为中心”(Notzero-centered output)。
2. 随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。这种神经元的死亡是不可逆转的死亡。



# 过拟合解决方法

1. 增加数据量
2. L正则式
	cost = (Wx - realY)^2 + abs(W)
	cost = (Wx - realY)^2 + (W)^2
3. dropout regularization
	神经网络中随机忽略一些节点


# 加速方法：

## momentum：

惯性原则

## AdaGrad

每个参数都有自己的学习率

## RMSProp

Momentum+AdaGrad

## Adam

# RNN







