---
layout: post
title: "Multi-LayerPerceptrons"
description: "Multi-Layer Perceptrons"
categories: [NLP]
tags: [Multi-LayerPer ceptrons]
redirect_from:
  - /2019/07/28/
---

* Karmdown table of content
{:toc .toc}

神经网络是一种运算模型，由大量的节点（称之为神经元）之间的相互连接构成，每个节点代表一种特定的输出函数，称之为激励函数（activation function）。没两个节点之间的连接代表一个通过该连接信号的权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。

https://blog.csdn.net/qq_44992589/article/details/89874259

人工神经网络的基本单元是多输入、单输出的模型。

在神经网络的学习过程中，神经元的传递函数和转换函数已经确定了，如果想要改变网络输出的大小，只能通过改变神经元的全职参数来实现。因此，神经网络的学习过程就是改变权值矩阵的过程。

神经网络的工作包括离线学习和在线判断两个部分。

常用学习规则：

1. 误差修正型规则：有监督。根据实际输出与目标输出之间的误差不断修正权值，最终达到误差小于目标函数的效果。包括δ学习规则、Widrow-Hoff学习规则、感知器学习规则和误差反向传播的BP(Back Propagation)学习规则等。

2. 竞争型规则：无监督。网络根据提供的一些学习样本进行自组织学习，没有期望输出，通过神经元相互竞争对外界刺激模式响应的权利进行网络权值的调整来适应输入的样本数据。 

3. Hebb型规则：利用神经元之间的活化值(激活值)来反映它们之间联接性的变化，即根据相互连接的神经元之间的活化值(激活值)来修正其权值。 在Hebb学习规则中，学习信号简单地等于神经元的输出。Hebb学习规则代表一种纯前馈﹑无导师学习。该学习规则至今在各种神经网络模型中起着重要作用。典型的应用如利用Hebb规则训练线性联想器的权矩阵。

4. 随机型规则：在学习过程中结合了随机、概率论和能量函数的思想，根据目标函数（即网络输出均方差）的变化调整网络的参数，最终使网络目标函数达到收敛值。

激活函数：神经元在输入信号作用下产生输出信号的规律由神经元功能函数f（Activation Function）给出，也称激活函数，或称转移函数。

常用激活函数形式：

1. 阈值函数：该函数通常也称为阶跃函数。当激活函数采用阶跃函数时，人工神经元模型即为MP模型。此时神经元的输出取１或０，反应了神经元的兴奋或抑制。

2. 线性函数：该函数可以在输出结果为任意值时作为输出神经元的激活函数，但是当网络复杂时，线性激活函数大大降低网络的收敛性，故一般较少采用。

3. 对数S形函数：对数S形函数的输出介于0~1之间，常被要求为输出在０～１范围的信号选用。它是神经元中使用最为广泛的激活函数。

4. 双曲正切S形函数：双曲正切S形函数类似于被平滑的阶跃函数，形状与对数S形函数相同，以原点对称，其输出介于-１~１之间，常常被要求为输出在-１~１范围的信号选用。

神经元的连接形式：

1. 前向网络（前馈网络）：第i层的神经元只接受第(i-1)层神经元给出的信号，各神经元之间没有反馈。有自适应线性神经网络(AdaptiveLinear，简称Adaline)、单层感知器、多层感知器、BP等。 

2. 反馈网络：每个节点都表示一个计算单元，同时接受外加输入和其它各节点的反馈输入，每个节点也都直接向外部输出。Hopfield、Hamming、BAM等即属此种类型。

# 前向网络：

## 单层感知器模型：

算法思想：首先把连接权和阈值初始化为较小的非零随机数，然后把有n个连接权值的输入送入网络，经加权运算处理，得到的输出如果与所期望的输出有较大的差别，就对连接权值参数按照某种算法进行自动调整，经过多次反复，直到所得到的输出与所期望的输出间的差别满足要求为止。

线性不可分问题：单层感知器不能表达的问题被称为线性不可分问题。 1969年，明斯基证明了“异或”问题是线性不可分问题。

线性不可分函数的数量随着输入变量个数的增加而快速增加，甚至远远超过了线性可分函数的个数。也就是说，单层感知器不能表达的问题的数量远远超过了它所能表达的问题的数量。

## 多层感知器模型：

在单层感知器的输入部分和输出层之间加入一层或多层处理单元，就构成了二层或多层感知器。

在多层感知器模型中，只允许某一层的连接权值可调，这是因为无法知道网络隐层的神经元的理想输出，因而难以给出一个有效的多层感知器学习算法。

多层感知器克服了单层感知器的许多缺点，原来一些单层感知器无法解决的问题，在多层感知器中就可以解决。例如，应用二层感知器就可以解决异或逻辑运算问题

## 反向传播模型（B-P模型）

算法思想：正向传播时，输入样本从输入层传入，经隐层逐层处理后，传向输出层。若输出层的实际输出与期望输出不符，则转向误差的反向传播阶段。

算法优缺点：

优点： 
理论基础牢固，推导过程严谨，物理概念清晰，通用性好等。所以，它是目前用来训练前向多层网络较好的算法。

缺点： 
(1)、该学习算法的收敛速度慢； 
(2)、网络中隐节点个数的选取尚无理论上的指导； 
(3)、从数学角度看，B-P算法是一种梯度最速下降法，这就可能出现局部极小的问题。当出现局部极小时，从表面上看，误差符合要求，但这时所得到的解并不一定是问题的真正解。所以B-P算法是不完备的。

局限性：

(1)、在误差曲面上有些区域平坦，此时误差对权值的变化不敏感，误差下降缓慢，调整时间长，影响收敛速度。这时误差的梯度变化很小，即使权值的调整量很大，误差仍然下降很慢。造成这种情况的原因与各节点的净输入过大有关。

(2)、存在多个极小点。从两维权空间的误差曲面可以看出，其上存在许多凸凹不平，其低凹部分就是误差函数的极小点。可以想象多维权空间的误差曲面，会更加复杂，存在更多个局部极小点，它们的特点都是误差梯度为0。BP算法权值调整依据是误差梯度下降，当梯度为0时，BP算法无法辨别极小点性质，因此训练常陷入某个局部极小点而不能自拔，使训练难以收敛于给定误差。

几种改进方法：

