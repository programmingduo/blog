---
layout: post
title: "GPTS"
description: ""
categories: [NLP]
tags: []
redirect_from:
  - /2023/03/21/
---

* Karmdown table of content
{:toc .toc}

# 基础知识

## 指示学习（Instruct Learning）和提示（Prompt Learning）学习

指示学习是谷歌Deepmind的Quoc V.Le团队在2021年的一篇名为《Finetuned Language Models Are Zero-Shot Learners》文章中提出的思想。指示学习和提示学习的目的都是去挖掘语言模型本身具备的知识。不同的是Prompt是激发语言模型的补全能力，例如根据上半句生成下半句，或是完形填空等。Instruct是激发语言模型的理解能力，它通过给出更明显的指令，让模型去做出正确的行动。

我们可以通过下面的例子来理解这两个不同的学习方式：

> 提示学习：给女朋友买了这个项链，她很喜欢，这个项链太____了。
> 指示学习：判断这句话的情感：给女朋友买了这个项链，她很喜欢。选项：A=好；B=一般；C=差。

指示学习的优点是它经过多任务的微调后，也能够在其他任务上做zero-shot，而提示学习都是针对一个任务的。泛化能力不如指示学习。我们可以通过图2来理解微调，提示学习和指示学习。

![smiley](\assets\images\usedInBlogs\GPTs\1.png)


## 人工反馈的强化学习

因为训练得到的模型并不是非常可控的，模型可以看做对训练集分布的一个拟合。那么反馈到生成模型中，训练数据的分布便是影响生成内容的质量最重要的一个因素。有时候我们希望模型并不仅仅只受训练数据的影响，而是人为可控的，从而保证生成数据的有用性，真实性和无害性。论文中多次提到了对齐（Alignment）问题，我们可以理解为模型的输出内容和人类喜欢的输出内容的对齐，人类喜欢的不止包括生成内容的流畅性和语法的正确性，还包括生成内容的有用性、真实性和无害性。

我们知道强化学习通过奖励（Reward）机制来指导模型训练，奖励机制可以看做传统模型训练机制的损失函数。奖励的计算要比损失函数更灵活和多样（AlphaGO的奖励是对局的胜负），这带来的代价是奖励的计算是不可导的，因此不能直接拿来做反向传播。强化学习的思路是通过对奖励的大量采样来拟合损失函数，从而实现模型的训练。同样人类反馈也是不可导的，那么我们也可以将人工反馈作为强化学习的奖励，基于人工反馈的强化学习便应运而生。

RLHF最早可以追溯到Google在2017年发表的《Deep Reinforcement Learning from Human Preferences》[6]，它通过人工标注作为反馈，提升了强化学习在模拟机器人以及雅达利游戏上的表现效果。


InstructGPT/ChatGPT中还用到了强化学习中一个经典的算法：OpenAI提出的最近策略优化（Proximal Policy Optimization，PPO）[7]。PPO算法是一种新型的Policy Gradient算法，Policy Gradient算法对步长十分敏感，但是又难以选择合适的步长，在训练过程中新旧策略的的变化差异如果过大则不利于学习。PPO提出了新的目标函数可以在多个训练步骤实现小批量的更新，解决了Policy Gradient算法中步长难以确定的问题。其实TRPO也是为了解决这个思想但是相比于TRPO算法PPO算法更容易求解。






# instructGPT

《Training language models to follow instructions with human feedback》

## 主要贡献：

提升效果：Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.

提升可靠性：InstructGPT models show improvements in truthfulness over GPT-3

在部分方面减少了有害性言论：InstructGPT shows small improvements in toxicity over GPT-3, but not bias

提高了通用性：We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure

Our models generalize to the preferences of “held-out” labelers that did not produce any training data

面对没有出现的人工引导的prompt， InstructGPT的结果也会更好：InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution

InstructGPT仍然会犯简单的小错误：InstructGPT still makes simple mistakes


## 训练步骤：

> 训练监督模型：Collect demonstration data, and train a supervised policy
> 训练奖励模型： Collect comparison data, and train a reward model.
> 利用奖励模型优化有监督模型

## 数据集：

fine-tuning 过程包括三类数据集，大约96%的数据均为英文：
> (1) our SFT dataset, with labeler demonstrations used to train our SFT models，contains about 13k training prompts (from the API and labeler-written)
> (2) our RM dataset, with labeler rankings of model outputs used to train our RMs has 33k training prompts (from the API and labeler-written)
> (3) our PPO dataset, without any human labels, which are used as inputs for RLHF fine-tuning, has 31k training prompts (only from the API)

其中，SFT数据集中各监督任务数据占比和RM数据集示例如图所示


![smiley](\assets\images\usedInBlogs\GPTs\2.png)


## 实验：

反正就是厉害，碾压。结论基本都在主要贡献里面了。

参考文献及博客： 
> Training language models to follow instructions with human feedback
> https://zhuanlan.zhihu.com/p/590311003


# GPT-4 Technical Report

整个报告有100页，真是牲口啊，这看完都五一了。

不过仔细一看，正文可能就一半左右的篇幅，剩下的是contrbutors和reference。窃喜。

## 可预测性

openAI根据神经网络特性，对硬件进行了改进，除加快了计算速度之外，GPT-4有一个惊为天人的改进：可以根据小模型预测参数量扩展后的模型效果。主要包括以下方面：

> GPT-4通过拟合scaling law，利用万分之一的算力计算出了最终的loss。
> 利用千分之一的算力预测了模型在HumanEval数据集的表现











参考文献及博客： 

https://arxiv.org/pdf/2303.08774.pdf