---
layout: post
title: "IE-paper"
description: ""
categories: [NLP]
tags: []
redirect_from:
  - /2020/04/16/
---

* Karmdown table of content
{:toc .toc}


# 前言

horovod的介绍可以看文末的参考链接。

整体训练过程中比较繁琐的是ssh的配置问题，一定要进行。有教程说需要配置hdfs，但是我没有配置也成功部署了。因为本教程是在docker上进行部署的，因此需要读者对docker有比较深入的了解。如果不了解docker的话可以再教程中出现docker的命令是自行百度一下。

# 训练过程

## 配置单机

直接使用[Horovod发布的Docker](https://hub.docker.com/r/horovod/horovod/tags?page=1&ordering=last_updated)。

本人选择版本为：0.18.1-tf1.14.0-torch1.2.0-mxnet1.5.0-py2.7。之后执行

~~~
docker pull horovod/horovod:0.18.1-tf1.14.0-torch1.2.0-mxnet1.5.0-py2.7
docker image ls |grep horovod // 查看image的id
nvidia-docker run -it --privileged --network=host --name horovod <id> // 将id进行替换
docker start <id>  // 将id进行替换
docker attach <id> // 将id进行替换
~~~

之后可以换源、简单配置vim等系统设置。



## 配置多机ssh

在多机上安装多个docker后，需要对每个docker配置ssh环境。这里以AB两个节点为例进行讲解。其中A为主节点，B为附节点。如果有更多节点，则与B配置相同。


先在B服务器上开启ssh
1. 修改sshd配置
vim /etc/ssh/sshd_config
​
2. 改动如下

~~~
Port 12345
PermitRootLogin yes
PubkeyAuthentication yes
AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2
~~~
​
3. 保存配置，启动sshd
/usr/sbin/sshd
​
4. 查看ssh是否启动
ps -ef | grep ssh
​

在A服务器上创建秘钥并且免密登录到B

1. 生成秘钥，一直回车即可，注意生成秘钥位置
ssh-keygen -t rsa
​
2. 在B上创建.ssh文件夹并将A中.ssh/id_rsa.pub复制到文件末

3. 测试是否可以免密登录
ssh -p 12345 B的ip


为了省去命令中每次都需要写host的IP地址，可以给他起个名字。在~/.ssh/config中输入：

~~~~
Host node237
    HostName ip1
    Port 12345

Host node238
    HostName ip2
    Port 12345

~~~~

## 部署bert

从[github](https://github.com/lambdal/bert)上下载代码。在各个节点中都要在相同位置存放BERT的参数配置。之后执行下列命令测试bert是否部署成功。如果程序正确执行，则表示部署成功。

~~~
BERT_BASE_DIR="/root/bert-base"

python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5

horovodrun --verbose \
    --start-timeout 1000 \
    -np 12 \
    -H node238:4,localhost:4,node237:4 \
    -p 12345 \
    python ./run_pretraining_hvd.py \
    --input_file=./tmp/tf_examples.tfrecord \
    --output_dir=./tmp/pretraining_output \
    --do_train=True \
    --do_eval=True \
    --bert_config_file=$BERT_BASE_DIR/bert_config.json \
    --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
    --train_batch_size=32 \
    --max_seq_length=128 \
    --max_predictions_per_seq=20 \
    --num_train_steps=20 \
    --num_warmup_steps=10 \
    --learning_rate=2e-5
~~~

其中，node237和node238可以替换为读者自己的机器ip。

另外，本人在执行过程中，需要把各个机器中output_dir参数路径下的文件删除干净，否则容易出bug。

## 准备训练数据

在网上的数据基本上都需要进行格式的转换和处理。最终的输入格式要求，每个文件大小尽量控制在几兆内，句子之间以换行隔开，文章中间夹入空行。

### CLUE

训练数据可以从[CLUE](https://github.com/CLUEbenchmark/CLUE) 中找，下载比较慢，可以充个会员。我当时等了一整天。那个webtext2019稍微快点，因为它小，可以优先保这个得下载速度。

可用的有两个：

新闻语料 news2016zh_corpus: 8G语料，分成两个上下两部分，总共有2000个小文件
社区互动-语料 webText2019zh_corpus：3G语料，包含3G文本，总共有900多个小文件。

### 微信文章：
https://github.com/nonamestreet/weixin_public_corpus
部分网络抓取的微信公众号的文章，目前数据大约3G，共71w+文章

~~~bash
cat articles.z* > all.zip
unzip all.zip
python jsonparser.py
~~~~


解析脚本jsonparser.py：
~~~python
import codecs
import json
import re


file_total = 1000
out_fs = []
for id in range(file_total):
    out_fs.append(codecs.open('weixin_' + str(id) + '.txt', 'w', encoding='utf-8'))


with codecs.open('articles.json', 'r', encoding='utf-8') as f:
    for id, line in enumerate(f):
  out_f = out_fs[id % file_total]
        try:
            text = json.loads(line)
            text = re.sub('\s', '', text['content'])
            out_f.write(text)
            out_f.write('\n\n')
        except:
            continue

~~~~

### wudao文集

https://data.wudaoai.cn/home

采用20多种规则从100TB原始网页数据中清洗得出最终数据集，注重隐私数据信息的去除，源头上避免GPT-3存在的隐私泄露风险；包含教育、科技等50+个行业数据标签，可以支持多领域预训练模型的训练。开源了200G，解压之后大概170G。

解析脚本：

~~~python
#/usr/bin/env python
# -*- coding:utf-8 -*- 
"""
###############################################################################
# File Name: tmp.py
# Author: wu Duo
# mail: wuduo@pku.edu.cn
# Created Time: Fri Jul  2 07:06:23 2021
##########################################################################
"""
import codecs
import json
import time
import random
import re
import os
import sys
from multiprocessing import Process,Queue
reload(sys)
sys.setdefaultencoding('utf-8')


def get_new_file(id):
    file_num_in_folder = 1000
    out_path = './txt/' + str(id // file_num_in_folder)
    if id % file_num_in_folder == 0:
        os.system('mkdir ' + out_path)

    out_file = os.path.join(out_path, str(id % file_num_in_folder) + '.txt')
    return codecs.open(out_file, 'w', encoding='utf-8'), out_file



def write2file(txt, out_f):
    txt = txt.encode('utf-8')
    txt = re.sub('\s', '', txt)
    txt = re.sub('(。|！|？|\?|!)', r'\1\n', txt)
    out_f.write(txt)
    out_f.write('\n')

def main():
    in_path = './WuDaoCorpus2.0_base_200G'
    out_num = 0
    out_file, out_name = get_new_file(out_num)
    out_num += 1
    file_num = 0
    for file in os.listdir(in_path):
        if file_num % 20 == 0:
            print('file num : ' + str(file_num))
        file_num += 1
        print(file)
        if not file.endswith('json'):
            continue
        file = os.path.join(in_path ,file)
        with codecs.open(file, 'r', encoding='utf-8') as in_f:
            datas = json.loads(in_f.read())
            for data in datas:
                txt = data['content']
                write2file(txt, out_file)
                if os.path.getsize(out_name) > 3 * 1024 * 1024:
                    out_file.close()
                    out_file, out_name = get_new_file(out_num)
                    out_num += 1

    if not out_file.closed:
        out_file.close()
    

if __name__ == "__main__":
    main()
~~~~


### wikipedia dumps 

https://dumps.wikimedia.org/zhwiki/20210601/

下载文件zhwiki-20210601-pages-articles-multistream.xml.bz2（2.2 GB），共210W+文章，解析后

之后需要使用Wikiextractor进行抽取。参考链接：[维基百科语料中的词语相似度探索](http://www.52nlp.cn/tag/wikiextractor)

执行下列语句：

~~~bash
git clone https://github.com/attardi/wikiextractor.git
cd wikiextractor/
sudo python setup.py install

python3 wikiextractor/WikiExtractor.py -o ./zhwiki -b 10M --json --processes 8 ../zhwiki-20210220-pages-articles-multistream.xml.bz2
~~~~

之后将会得到json格式的文章。接着需要处理成bert输入的格式并且转换为简体字。处理脚本如下。命令：

~~~bash
python transfer.py zhwiki/
~~~~

~~~python
#/usr/bin/env python
# -*- coding:utf-8 -*- 
"""
###############################################################################
# File Name: transfer.py
# Author: wu Duo
# mail: wuduo@pku.edu.cn
# Created Time: Wed Jun 30 02:22:24 2021
##########################################################################
"""
import codecs
import json
import sys
import re
import os
from contextlib import nested
reload(sys)
sys.setdefaultencoding('utf-8')
s
def main(data_path):
    for root, dirs, files in os.walk(data_path):
        # print(root)
        for file in files:
            file_path = os.path.join(root, file)
            if file_path.endswith('out') or file_path.endswith('txt'):
                continue
            with codecs.open(file_path, 'r', encoding='utf-8') as in_f, \
                    codecs.open(file_path + '_out', 'w', encoding='utf-8') as out_f:
                for id, line in enumerate(in_f):
                    try:
                        data = json.loads(line)
                        txts = data['text'].encode('utf-8')
                        if not txts:
                            continue
                        txts = txts.split('\n')
                        for t in txts:
                            if t.endswith('.') or not t:
                                continue
                            t = re.sub('(。|！|？|\?|!)', r'\1\n', t)
                            out_f.write(t)
                        out_f.write('\n')
                    except Exception, e:
                        print(str(e))
                        print(file_path)
                        print(line.encode('utf-8'))
                        print(id)
            os.system('opencc -i {} -o {} -c t2s.json&'.format(file_path + '_out', file_path + '.txt'))
            print(file_path)


if __name__ == "__main__":
    main(sys.argv[1])


~~~~


### 百科问答类

https://github.com/brightmart/nlp_chinese_corpus

含有150万个预先过滤过的、高质量问题和答案，每个问题属于一个类别。总共有492个类别，其中频率达到或超过10次的类别有434个。使用过程中仅使用回答部分。

解析脚本：
~~~python
import codecs
import re
import json
import sys
from contextlib import nested
reload(sys)
sys.setdefaultencoding('utf-8')



def step1():

    def parser(f):
      for line in f:
        try:
          data = json.loads(line)
          text = data['answer']
          text = re.sub('\s', '', text)
          out_f.write(text)
          out_f.write('\n')
        except:
          continue

    with codecs.open('baike_qa', 'w', encoding='utf-8') as out_f:
      with codecs.open('baike_qa_valid.json', 'r', encoding='utf-8') as f:
        parser(f)
      with codecs.open('baike_qa_train.json', 'r', encoding='utf-8') as f:
        parser(f)


def step2():
    file_total = 300
    out_fs = []
    for id in range(file_total):
        out_fs.append(codecs.open('baike_qa_' + str(id) + '.txt', 'w', encoding='utf-8'))

    with codecs.open('baike_qa', 'r', encoding='utf-8') as f:
        for id, line in enumerate(f):
            id %= file_total
            text = line.encode('utf-8')
            text = re.sub('(。|！|？|\?|!)', r'\1\n', text)
            out_fs[id].write(text)
            out_fs[id].write('\n')
    
    for f in out_fs:
        f.close()


step1()
step2()

~~~~



### 新闻语料json版(news2016zh)

https://github.com/brightmart/nlp_chinese_corpus

250万篇新闻(原始数据9G，压缩文件3.6G；新闻内容跨度：2014-2016年)

包含了250万篇新闻。新闻来源涵盖了6.3万个媒体，含标题、关键词、描述、正文。

数据集划分：数据去重并分成三个部分。训练集：243万；验证集：7.7万；测试集，数万，不提供下载。


解析脚本：
~~~python
# encoding:utf-8
import codecs
import re
import json
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

def parser(f):
    for id, line in enumerate(f):
        out_f = out_fs[id % file_total]
        try:
            data = json.loads(line)
            text = data['content'].encode('utf-8')
            text = re.sub('\s', '', text)
            text = re.sub('(。|！|？|\?|!)', r'\1\n', text)
            out_f.write(text)
            out_f.write('\n')
        except Exception, e:
            print(e)
            continue

file_total = 3000
out_fs = []
for id in range(file_total):
    out_fs.append(codecs.open('baike_qa_' + str(id) + '.txt', 'w', encoding='utf-8'))

with codecs.open('news2016zh_valid.json', 'r', encoding='utf-8') as f:
    parser(f)
with codecs.open('news2016zh_train.json', 'r', encoding='utf-8') as f:
    parser(f)

~~~~


### 知网

https://web05.cnki.net/kns/brief/result.aspx?dbprefix=AKSJ

近十年期刊：7w+，学位论文1.9w，文献76条，报纸105条。总计约10w

https://kns.cnki.net/kns8/defaultresult/index
航空主题：1w+

### 航空类杂志

http://hanhaiqikan.cn/qikan/hangkong/index_3.html  47个杂志

https://www.avic.com.cn/sycd/xwzx/qkzz/?PC=PC?PC=PC  中国航空报3.6K期

https://zhuanlan.zhihu.com/p/269492874  约30份杂志

https://www.xueshu.com/qikan/hangkong/guojiajie/  约20份杂志

### 航空类书籍

一本书大概10M 


## 数据预处理

由于输入和输出都是多文件，所以要对源代码稍微进行一点改动。改动部分如下：

~~~python

def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)

  tokenizer = tokenization.FullTokenizer(
      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)

  allinput_files = []
  for input_pattern in FLAGS.input_file.split(","):
    allinput_files.extend(tf.gfile.Glob(input_pattern))
  alloutput_files = []
  for output_pattern in FLAGS.output_file.split(",")[:-1]:
    alloutput_files.append(output_pattern)
  
  assert(len(allinput_files) == len(alloutput_files))

  step = 20
  for id in range(0, len(allinput_files), step):
    print("id: " + str(id))
    input_files = allinput_files[id: id + step]
    output_files = alloutput_files[id: id + step]
    tf.logging.info("*** Reading from input files ***")
    for input_file in input_files:
      tf.logging.info("  %s", input_file)

    rng = random.Random(FLAGS.random_seed)
    instances = create_training_instances(
        input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,
        FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,
        rng)

    tf.logging.info("*** Writing to output files ***")
    for output_file in output_files:
      tf.logging.info("  %s", output_file)

    write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,
                                    FLAGS.max_predictions_per_seq, output_files)
  print('finish')
~~~~




## 训练bert


执行命令
~~~
BERT_BASE_DIR="/root/bert_para/chinese_L-12_H-768_A-12/"
DATA_DIR="/root/data/news"
OUT_DIR="/root/bert/data/news"

input_files=''
out_files=''
for file in  $(ls $DATA_DIR)
do
    input_files="$DATA_DIR/$file,$input_files"
    out_files="$OUT_DIR/$file,$out_files"
done

nohup python create_pretraining_data.py \
  --input_file=$input_files \
  --output_file=$out_files \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5 \
  > ./log/news_create_data 2> ./log/news_log_create_data &
~~~~






# 参考博客

[官方github](https://github.com/horovod/horovod)

[理论讲解](https://fyubang.com/2019/07/08/distributed-training/)

https://zhuanlan.zhihu.com/p/341922650

https://zhuanlan.zhihu.com/p/368539089

https://zhuanlan.zhihu.com/p/205173013