---
layout: post
title: "pytorch"
description: ""
categories: []
tags: []
redirect_from:
  - /2020/04/05/
---

* Karmdown table of content
{:toc .toc}

# 初始化相关

## 张量

~~~python
#torch.eye(shape) #生成对角线元素为1的对角矩阵
s = torch.eye(3, 2)
print(s)
'''
tensor([[1., 0.],
        [0., 1.],
        [0., 0.]])
'''



#torch.randn(*sizes, out=None) → Tensor
#返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。
#torch.rand(*sizes, out=None) → Tensor
#从区间[0, 1)的均匀分布中抽取的一组随机数
a = torch.rand(2, 2)
print(a)
'''
tensor([[0.4651, 0.1376],
        [0.9071, 0.2037]])
'''

a = torch.from_numpy(np.ones((2, 2), dtype=np.int))
b = torch.from_numpy(np.ones((2, 2), dtype=np.int)) * 2
#对应元素相加
print(a + b)
print(torch.add(a, b))
#每个元素加5
print(a.add(5))
#对应元素相乘
print(a * b）
print(a.mul(b))
#矩阵乘法
print(a.matmul(b))
'''
tensor([[8, 8],
        [8, 8]])
'''


~~~~


## 变量

变量封装了张量对象、梯度以及创建张量的函数引用

~~~python



~~~~

# layer

~~~python

from torch.nn import Linear

~~~~

# 常用函数

## batch相关

~~~python 
import torch
import torch.utils.data as Data

x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)
#线性等分向量

BATCH_SIZE = 5


torch_dataset = Data.TensorDataset(x, y)

loader = Data.DataLoader(
	dataset=torch_dataset,
	batch_size=BATCH_SIZE,
	shuffle=True,
	num_workers=2,
)

for epoch in range(3):
	for step, (batch_x, batch_y) in enumerate(loader):
		print(batch_x.numpy(), '|', batch_y.numpy())

~~~~

## Optimizer相关

~~~python
#-*- encoding:utf-8 -*-

import torch
import torch.utils.data as Data
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim = 1)
#给指定位置加上维数为一的维度
y = x.pow(2) + 0.1 * torch.normal(torch.zeros(*x.size()))

plt.scatter(x.numpy(), y.numpy())
#绘制散点图
plt.show()

torch_dataset = Data.TensorDataset(x, y)
loader = Data.DataLoader(
	dataset = torch_dataset,
	batch_size = BATCH_SIZE, 
	shuffle = True,
	num_workers = 2,
	)

class Net(torch.nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.hidden = torch.nn.Linear(1, 20)
		#对传入数据应用线性变换
		self.predict = torch.nn.Linear(20, 1)

	def forward(self, x):
		x = F.relu(self.hidden(x))
		x = self.predict(x)
		return x

net_SGD = Net()
net_Momentum = Net()
net_RSprop = Net()
net_Adam = Net()
nets = [net_SGD, net_Momentum, net_RSprop, net_Adam]

opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)
opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = 0.8)
opt_RMSprop = torch.optim.RMSprop(net_RSprop.parameters(), lr = LR, alpha = 0.9)
opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas = (0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

loss_func = torch.nn.MSELoss()
#均方损失
losses_his = [[], [], [], []]

for epoch in range(EPOCH):
	print(epoch)
	for step, (batch_x, batch_y) in enumerate(loader):
		b_x = Variable(batch_x)
		b_y = Variable(batch_y)
		for net, opt, l_his in zip(nets, optimizers, losses_his):
		#zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
			output = net(b_x)#get output for every net
			loss = loss_func(output, b_y)#get loss for every net
			opt.zero_grad()#clear gradients
			loss.backward()#backpropagation, comput gradients
			opt.step()#apply gradients
			l_his.append(loss)#loss recoder

labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']
for i, l_his in enumerate(losses_his):
	plt.plot(l_his, label = labels[i])
plt.legend(loc = 'best')
plt.xlabel('steps')
plt.ylabel('loss')
plt.ylim((0, 0.2))
plt.show()
~~~~~

图片1

# CNN

注：代码未调试，主要是数据集下载的时候出现了问题

~~~python 
#-*- encoding:utf-8 -*-

import torch
import torch.utils.data as Data
import torch.nn as nn
import torchvision
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12
DOWNLOAD_MINST = True

train_data = torchvision.datasets.MNIST(
	root='.',
	train = True,
	transform=torchvision.transforms.ToTensor(),
	download=DOWNLOAD_MINST,
)

class CNN(nn.Module):
	def __init__(self):
		super(CNN, self).__init__()
		self.conv1 = nn.Sequential(
			nn.Conv2d( #1, 28, 28
				in_channels=1,#输入图片的高度
				out_channels=16, #卷积之后的高度，也就是filter的个数
				kernel_size=5, #卷积核的长和宽
				stride=1, #步进为1
				padding=2, #原图的边缘扩充0已进行边缘数据与filter的相乘
				#if strid=1, padding=(kernel_size-1)/2 = (5-1)/2=2
			)，#->16,28, 28
			nn.ReLU(),
			nn.MaxPool2d(kernel_size=2), #->16, 14, 14
		)
		self.conv2 = nn.Sequential(
			nn.Conv2d(#16, 14, 14
				in_channels=16,#输入图片的高度
				out_channels=32, #卷积之后的高度，也就是filter的个数
				kernel_size=5, #卷积核的长和宽
				stride=1, #步进为1
				padding=2, #原图的边缘扩充0已进行边缘数据与filter的相乘
				#if strid=1, padding=(kernel_size-1)/2 = (5-1)/2=2
			)，#->32, 14, 14
			nn.ReLU(),
			nn.MaxPool2d(kernel_size=2),#->32, 7, 7
		)
		self.out = nn.Linear(32*7*7, 10)

	def forward(self, x):
		x = self.conv1(x)
		x = self.conv2(x) #(batch, 32, 7, 7)
		x = x.view(x.size(0), -1) #(batch, 32*7*7)
		output = self.out(x)
		return output

cnn = CNN()
print(cnn)

optimizer = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas = (0.9, 0.99))
loss_func = nn.CrossEntropyLoss()

for epoch in range(EPOCH):
	print(epoch)
	for step, (batch_x, batch_y) in enumerate(loader):
		b_x = Variable(batch_x)
		b_y = Variable(batch_y)
		output = cnn(b_x)#get output for every net
		loss = loss_func(output, b_y)#get loss for every net
		optimizer.zero_grad()#clear gradients
		loss.backward()#backpropagation, comput gradients
		optimizer.step()#apply gradients




~~~~

# embedding

~~~python 
# -*- encoding:utf-8 -*-
from torchtext import data
from torchtext import datasets
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torchtext.vocab import GloVe

TEXT = data.Field(lower=True, batch_first=True, fix_length=20)
LABEL = data.Field(sequential=False)
train, test = datasets.IMDB.splits(TEXT, LABEL)

TEXT.build_vocab(train, vectors=GloVe(name="6B", dim=50), max_size=10000, min_freq=10)
LABEL.build_vocab(train)

# print(len(train))
# print(vars(train[0]))
# print(train[0].text)
# print(test[0])

class EmbNet(nn.Module):
	def __init__(self, emb_size, sen_length, hidden_size1):
		super().__init__()
		self.hidden_size2 = sen_length * hidden_size1
		self.embedding = nn.Embedding(emb_size, hidden_size1)
		#embedding接受两个参数：词表大小 和 希望每个单词创建的维度
		self.fc = nn.Linear(self.hidden_size2, 3)

	def forward(self, x):#x: 32*20
		# print(x[0])
		#对于词表中没有出现的词人为把id改为0。
		for id1, t in enumerate(x):
			for id2, tt in enumerate(t):
				if tt >= 10000:
					x[id1][id2] = 0
			# print(t)
		embs =  self.embedding(x) #32*20*hidden_size1
		out = self.fc(embs.view(x.size(0), -1)) #
		out = F.dropout(out, p=0.7, training=self.training)
		return F.log_softmax(out, dim = -1)
		#对softmax进行了一次log操作


def fit(model, data_loader, epoch, phase='training', volatile=False, is_cuda=False):
	if phase == 'training':
		model.train()
	if phase == 'validation':
		model.eval()
		volatile = True

	running_loss = 0.0
	running_correct = 0
	optimizer = optim.Adam(model.parameters(), lr=1e-3)
	for batch_idx, batch in enumerate(data_loader):
		txt, target = batch.text, batch.label
		if is_cuda:
			txt, target = txt.cude(), target.cuda()
		if phase == 'training':
			optimizer.zero_grad()

		output = model(txt)
		loss = F.nll_loss(output, target)
		#可能损失负对数
		running_loss += F.nll_loss(output, target, size_average=False).data.item()
		#????
		preds = output.data.max(dim = 1, keepdim= True)[1]
		running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()
		if phase == 'training':
			loss.backward()
			optimizer.step()

	loss = running_loss / len(data_loader.dataset)
	accuracy = 100.0 * running_correct / len(data_loader.dataset)
	print('epoch: %d, loss: %f, accuracy: %f' %(epoch, loss, accuracy))
	return loss, accuracy

model = EmbNet(10000, 20, 50)
train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size = 32, device = -1, shuffle = True)
train_iter.repeat = False
test_iter.repeat = False##

for epoch in range(1, 10):
	epoch_loss, epoch_accuracy = fit(model, train_iter, epoch, phase='training')
	val_epoch_loss, val_epoch_accuracy = fit(model, test_iter, epoch, phase = 'validation')

#整个过程准确率在64左右收敛，效果不好，过拟合严重

~~~~
