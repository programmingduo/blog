---
layout: post
title: "pytorch"
description: ""
categories: []
tags: []
redirect_from:
  - /2020/04/05/
---

* Karmdown table of content
{:toc .toc}

# 初始化相关

## 张量

~~~python
#torch.eye(shape) #生成对角线元素为1的对角矩阵
s = torch.eye(3, 2)
print(s)
'''
tensor([[1., 0.],
        [0., 1.],
        [0., 0.]])
'''



#torch.randn(*sizes, out=None) → Tensor
#返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义。
#torch.rand(*sizes, out=None) → Tensor
#从区间[0, 1)的均匀分布中抽取的一组随机数
a = torch.rand(2, 2)
print(a)
'''
tensor([[0.4651, 0.1376],
        [0.9071, 0.2037]])
'''

a = torch.from_numpy(np.ones((2, 2), dtype=np.int))
b = torch.from_numpy(np.ones((2, 2), dtype=np.int)) * 2
#对应元素相加
print(a + b)
print(torch.add(a, b))
#每个元素加5
print(a.add(5))
#对应元素相乘
print(a * b）
print(a.mul(b))
#矩阵乘法
print(a.matmul(b))
'''
tensor([[8, 8],
        [8, 8]])
'''


~~~~


## 变量

变量封装了张量对象、梯度以及创建张量的函数引用

~~~python



~~~~

# layer

~~~python

from torch.nn import Linear

~~~~

# 常用函数

## batch相关

~~~python 
import torch
import torch.utils.data as Data

x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)
#线性等分向量

BATCH_SIZE = 5


torch_dataset = Data.TensorDataset(x, y)

loader = Data.DataLoader(
	dataset=torch_dataset,
	batch_size=BATCH_SIZE,
	shuffle=True,
	num_workers=2,
)

for epoch in range(3):
	for step, (batch_x, batch_y) in enumerate(loader):
		print(batch_x.numpy(), '|', batch_y.numpy())

~~~~

## Optimizer相关

~~~python
#-*- encoding:utf-8 -*-

import torch
import torch.utils.data as Data
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12

x = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim = 1)
#给指定位置加上维数为一的维度
y = x.pow(2) + 0.1 * torch.normal(torch.zeros(*x.size()))

plt.scatter(x.numpy(), y.numpy())
#绘制散点图
plt.show()

torch_dataset = Data.TensorDataset(x, y)
loader = Data.DataLoader(
	dataset = torch_dataset,
	batch_size = BATCH_SIZE, 
	shuffle = True,
	num_workers = 2,
	)

class Net(torch.nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.hidden = torch.nn.Linear(1, 20)
		#对传入数据应用线性变换
		self.predict = torch.nn.Linear(20, 1)

	def forward(self, x):
		x = F.relu(self.hidden(x))
		x = self.predict(x)
		return x

net_SGD = Net()
net_Momentum = Net()
net_RSprop = Net()
net_Adam = Net()
nets = [net_SGD, net_Momentum, net_RSprop, net_Adam]

opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr = LR)
opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr = LR, momentum = 0.8)
opt_RMSprop = torch.optim.RMSprop(net_RSprop.parameters(), lr = LR, alpha = 0.9)
opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas = (0.9, 0.99))
optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]

loss_func = torch.nn.MSELoss()
#均方损失
losses_his = [[], [], [], []]

for epoch in range(EPOCH):
	print(epoch)
	for step, (batch_x, batch_y) in enumerate(loader):
		b_x = Variable(batch_x)
		b_y = Variable(batch_y)
		for net, opt, l_his in zip(nets, optimizers, losses_his):
		#zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。
			output = net(b_x)#get output for every net
			loss = loss_func(output, b_y)#get loss for every net
			opt.zero_grad()#clear gradients
			loss.backward()#backpropagation, comput gradients
			opt.step()#apply gradients
			l_his.append(loss)#loss recoder

labels = ['SGD', 'Momentum', 'RMSprop', 'Adam']
for i, l_his in enumerate(losses_his):
	plt.plot(l_his, label = labels[i])
plt.legend(loc = 'best')
plt.xlabel('steps')
plt.ylabel('loss')
plt.ylim((0, 0.2))
plt.show()
~~~~~

图片1

## CNN相关

注：代码未调试，主要是数据集下载的时候出现了问题

~~~python 
#-*- encoding:utf-8 -*-

import torch
import torch.utils.data as Data
import torch.nn as nn
import torchvision
import torch.nn.functional as F
from torch.autograd import Variable
import matplotlib.pyplot as plt

LR = 0.01
BATCH_SIZE = 32
EPOCH = 12
DOWNLOAD_MINST = True

train_data = torchvision.datasets.MNIST(
	root='.',
	train = True,
	transform=torchvision.transforms.ToTensor(),
	download=DOWNLOAD_MINST,
)

class CNN(nn.Module):
	def __init__(self):
		super(CNN, self).__init__()
		self.conv1 = nn.Sequential(
			nn.Conv2d( #1, 28, 28
				in_channels=1,#输入图片的高度
				out_channels=16, #卷积之后的高度，也就是filter的个数
				kernel_size=5, #卷积核的长和宽
				stride=1, #步进为1
				padding=2, #原图的边缘扩充0已进行边缘数据与filter的相乘
				#if strid=1, padding=(kernel_size-1)/2 = (5-1)/2=2
			)，#->16,28, 28
			nn.ReLU(),
			nn.MaxPool2d(kernel_size=2), #->16, 14, 14
		)
		self.conv2 = nn.Sequential(
			nn.Conv2d(#16, 14, 14
				in_channels=16,#输入图片的高度
				out_channels=32, #卷积之后的高度，也就是filter的个数
				kernel_size=5, #卷积核的长和宽
				stride=1, #步进为1
				padding=2, #原图的边缘扩充0已进行边缘数据与filter的相乘
				#if strid=1, padding=(kernel_size-1)/2 = (5-1)/2=2
			)，#->32, 14, 14
			nn.ReLU(),
			nn.MaxPool2d(kernel_size=2),#->32, 7, 7
		)
		self.out = nn.Linear(32*7*7, 10)

	def forward(self, x):
		x = self.conv1(x)
		x = self.conv2(x) #(batch, 32, 7, 7)
		x = x.view(x.size(0), -1) #(batch, 32*7*7)
		output = self.out(x)
		return output

cnn = CNN()
print(cnn)

optimizer = torch.optim.Adam(net_Adam.parameters(), lr = LR, betas = (0.9, 0.99))
loss_func = nn.CrossEntropyLoss()

for epoch in range(EPOCH):
	print(epoch)
	for step, (batch_x, batch_y) in enumerate(loader):
		b_x = Variable(batch_x)
		b_y = Variable(batch_y)
		output = cnn(b_x)#get output for every net
		loss = loss_func(output, b_y)#get loss for every net
		optimizer.zero_grad()#clear gradients
		loss.backward()#backpropagation, comput gradients
		optimizer.step()#apply gradients




~~~~
