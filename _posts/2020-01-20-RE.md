---
layout: post
title: "RE-paper"
description: ""
categories: [NLP]
tags: []
redirect_from:
  - /2020/01/20/
---

* Karmdown table of content
{:toc .toc}

# 《Few-shot Learning: A Survey》

一篇40页的巨大综述作者介绍完表示、问题定义、相关学习、核心问题、分类方法之后，从数据、模型、算法三个角度开始论述。

![smiley](\assets\images\usedInBlogs\RE\4.png)

## 数据

作者将数据增强的方法分为四个类别，如下图所示：

![smiley](\assets\images\usedInBlogs\RE\3.png)

### duplicate data with transformation

有两种方法可供需选择：人工构造和从其他任务的数据中转换。

### borrow from other data set

similar data set 是一种神奇的方式，没看懂。。。

## model

通过先验知识减小参数空间，达到FSL的目的。作者把利用先验知识的模型分为5类（很奇怪，论文说5类，图片只有4类），如下图所示：

![smiley](\assets\images\usedInBlogs\RE\5.png)

### multitask

通常人物之间是相关联的。跨领域的多任务也称作 domain adaptation。根据参数共享是否强制进行将多任务方法划分为hard and soft parameter sharing

下图展示了两种方法

![smiley](\assets\images\usedInBlogs\RE\6.png)

#### hard parameter sharing 

有一种方法是不同的task之间共享前几层，每个任务有各自的最后一层。同时也*有一种方法可以选取source and target task最相关的sample*

另一种完全不同的domain adaptation方法，每个任务各自学习embedding，之后共享一个分类器。

#### soft parameter sharing

不共享参数，只是鼓励各任务之间的参数保持相似，比如通过正则实现。如果不同task之间的关系已经给出，name正则就可以变成一个图拉普拉卡正则。

另一种不通过正则的方法是在source task中train一个CNN，用来初始化target task，在训练过程中使用了adversarial loss（没看懂），同时也利用未标注的data来做数据增强

#### conclusion

相比而言，hard的方式更适合相似task，soft的方式则更加灵活。

### embedding learning

在测试机和训练集分别训练一个embedding function。本文将embedding learning划分为 task-specific 和 task-invariant 以及两者的结合。

![smiley](\assets\images\usedInBlogs\RE\7.png)

#### Task-specific

将数据处理成pair，因此每个例子都会被处理成多个pair的input，大大降低了监督信息中的样例需求。embedding要学习的就是两个样例是否是同样的y。

#### Task-invariant

这种方式要先从本任务以外的其他任务的数据集中学习。其实引入了预训练的思路。在这种embedding learning中也有利用pair进行孪生网络（siamese net）的学习。

#### Combine Task-specific and Task-invariant

这部分没有仔细看

### Learning with External Memory

![smiley](\assets\images\usedInBlogs\RE\15.png)

![smiley](\assets\images\usedInBlogs\RE\16.png)

比如神经图灵机（NTM），memory networks。看一篇相关的论文吧，[《Memory Matching Networks for One-Shot Image Recognition》]()。

看起来像是直接对样例进行相似度计算，像是孪生网络，可是原文中说是常用的是LSTM、RNN，很迷。没仔细看。

这种方法的重难点在于设计记忆替换的方法

### Generative Model

![smiley](\assets\images\usedInBlogs\RE\17.png)

利用先验知识学习模型参数的概率分布，之后利用FSL的数据集确定参数，求概率。看一篇相关论文吧。[《2017  Towards a Neural Statistician》]()，[《2019  Meta-Learning Probabilistic Inference for Prediction.》]()。











# 《Memory Matching Networks for One-Shot Image Recognition》

## introduction

预训练网络的缺陷：预训练数据和实际数据来自很不同的领域则会导致很差的结果；实际数据可能会少到很快就过拟合。

解决方法：1. 训练的时候每一个batch的数据中，每一个类别都只有一个或者很少的样例。这样训练的时候就模拟了少样本的环境
2. 加入了 memory 模块，将数据集 压缩并且泛化到slot，并且产生support set整体的结果
3. 将memory slots 喂给RNN，用来预测未标注的图片的CNN参数。

![smiley](\assets\images\usedInBlogs\RE\18.png)




# 《Attention Guided Graph Convolutional Networks for Relation Extraction》(ACL 2019)

论文地址：https://arxiv.org/abs/1906.07510

GitHub地址：https://github.com/Cartus/AGGCN_TACRED

## 结构：

![smiley](\assets\images\usedInBlogs\RE\8.png)

The AGGCN model is shown with an example sentence and its dependency tree. It is composed of M identical blocks and each block has three types of layers as shown on the right. Every block takes node embeddings and adjacency matrix that represents the graph as inputs. Then N attention guided adjacency matrices are constructed by using multi-head attention as shown at bottom left. The original dependency tree is transformed into N different fully connected edge-weighted graphs (self-loops are omitted for simpliﬁcation). Numbers near the edges represent the weights in the matrix. Resulting matrices are fed into N separate densely connected layers, generating new representations. Top left shows an example of the densely connected layer, where the number (L) of sub-layers is 3 (L is a hyper-parameter). Each sub-layer concatenates all preceding outputs as the input. Eventually, a linear combination is applied to combine outputs of N densely connected layers into hidden representations.

首先利用GCN获得句子的encoding。而为了考虑硬剪枝带来的信息损失，加入了attention层，与GCN共同得到句子的encoding。为了在关系抽取中蝴蝶更好的效果，实际操作中在获得句子encoding的过程中使用了mask，也就是将句子中去除entity之后的部分做了encoding。将句子的encoding和entity的encoding连接起来之后获得整个encoding。将这个encoding作为输入，调用FFNN模型，得到最后的分类结果，也就是关系。

### GCN

Thomas N. Kipf and Max Welling. 2017. Semisupervised classiﬁcation with graph convolutional networks. In Proc. of ICLR.

Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proc. of EMNLP.

### Attention guided layer

现有的剪枝方法会导致在结果子树上没有边的节点之间权重会直接赋值为0，可能会导致原来依赖树中的有效信息丢失（比如结果子树带有的误差和多跳路径）。而attention guided layer则可以克服这一缺点。

attention层会把树转换成一个全联通的矩阵A，A中元素的值代表节点之间的关系的权重。

A通过self-attention mechanism（Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.Long short-term memory-networks for machine reading. In Proc. of EMNLP.）构造。
之后通过multi-head attention（Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NeurIPS.）进行计算、更新。计算公式如下图：


![smiley](\assets\images\usedInBlogs\RE\9.png)

在实际操作中，作者用原来的邻接矩阵作为attention层的初始化。从第二个block开始，attention guided layer开始正式引入。



### Densely Connected Layer

为了获得更加结构化的信息，引入了densely connected layer


# FewRel 2.0: Towards More Challenging Few-Shot Relation Classification

github地址：https://github.com/thunlp/fewrel

## 阅读原因

文章尝试解决*训练数据不足的*情况下，也就是在*小规模训练数据*条件下抽取关系。

## 总体概述

论文中作者并未讲述很多关于实现的内容，主要描述了实验数据集的组成和训练、测试集的构建方法。具体实现内容还得看github。

## 文章关注的两个问题点：

### 1. Few-shot domain adaptation

文章着手于提高few-shot模型的DA能力，及在一个全新的、训练数据集中没有的领域，关系抽取能力是否依然强健。

### 2. Few-shot none-of-the-above detection

文章着手的另一个问题就是NOTA问题，即在面对NOTA关系时是否能够正确区分。这一问题的实验是建立在N-wayK-shot的基础上完成的。可以理解为N+1wayK-shot

## 数据集构建

1. 人工标注了在Wikipedia corpus and Wikidata上执行远程监督抽取方法的结果(Bunescu and
Mooney, 2007; Mintz et al., 2009)

2. 验证DA能力的测试集中，加入了PubMed和UMLS数据集（生物制药领域）中的知识并进行人工标注，最终形成了25种关系，每种关系100个实例的测试集。

3. 验证NOTA关系检测的能力时，需要引入实体之间没有关系的实例和实体之间的关系不在训练集中的实例。作者引入NOTA 率α表示NOTA关系的实例占比。假定训练数据集中关系种类有N
种，测试数据集除了需要覆盖这N种关系之外还需要另外抽样训练数据中未涵盖的关系，抽取个数取决于α的设置。

## 解决DA问题的方法

采用对抗训练的方法。

一方面，句子的encoder需要尽量减小不同领域之间的差异。而另一方面，分类器则需要尽量将不同领域的关系分离开。因此适用于对抗训练。用于训练对抗网络的数据来自training domain和test domain的文档。

分类器采用一个两层的perceptron network。

对抗的min-max关系如下：

![smiley](\assets\images\usedInBlogs\RE\10.png)

## 解决NOTA问题的方法

基于bert的序列分类模型提出了BERT-PAIR。将测试实例和每一个训练实例连接起来，形成一个pair并视为一个sequence。将这个sequence交给bert的序列分类模型，得到这两个实例表示同一种关系的分数。预测结果的可能性表示为：

![smiley](\assets\images\usedInBlogs\RE\11.png)

其中，

![smiley](\assets\images\usedInBlogs\RE\12.png)

而NOTA的分数则表示为下述公式：


![smiley](\assets\images\usedInBlogs\RE\13.png)

## 实验

### baseline models

encoder：CNN encoder/BERT
model：GNN / prototypical networks

### DA

实验结果如下所示

![smiley](\assets\images\usedInBlogs\RE\14.png)

FewRel2.0的测试集中增加了新领域数据，可以更好地表达DA的能力。而1.0则代表机器学习新领域关系抽取所能够达到的理论最佳效果。

从上表可以看出：

1. 所有的few-shot模型在新的domain中表现都会很差
2. 对抗网络确实有利于提高新的domain中抽取效果。
3. BERT-PAIR真香。。。

### NOTA

(1) Treating NOTA as the N +1 relation is beneficial for handling Few-Shot NOTA, though the results still fall fast when the NOTA rate increases.
(2) BERT-PAIR works better under the NOTA setting for its binary-classification style model, and stays stable with rising NOTA rate.
(3) Though BERT-PAIR achieves promising results, huge gaps still exist between the conventional (0% NOTA rate) and NOTA settings (gaps of 8 points for 5-way 1-shot and 7 points for 5- way 5-shot with 50% NOTA rate), which calls for further research to address the challenge

# 《Neural Snowball for Few-Shot Relation Learning AAAI20》

## 阅读原因

清华大四特奖的论文。也是尝试解决*训练数据不足的*情况下，也就是在*小规模训练数据*条件下抽取关系。


## 概述

整体流程中分为两个部分，基于bootstrapping、transfer learning提出的Neural Snowball，利用RSN和关系分类器扩充样本；relational Siamese networks，用来判定两个语句是否表示同一关系。

## neural snowball

### phase1 扩展表示关系的句子

在无标注数据集中包含特定实体对的所有句子中，使用RSN判定该语句是否与标注过此实体对表示某关系的语句表示的是同一种关系。用得到的评分的平均值表示无标注语句的最终结果。取前K个并且平均分大于某阈值的数据，作为下次的标注数据。


### phase2 扩展实体对

与上一步相似，只不过RSN换做了一个关系判别器。

## Neural Modules

主要就是上一节中提到的两个，RSN和关系判别器

### RSN

包含encoder和distant function。

![smiley](\assets\images\usedInBlogs\RE\1.png)

### Relation Classifier

包含encoder和N个二分类器。最后取二分类器中取值最高的结果作为最终分类结果。

![smiley](\assets\images\usedInBlogs\RE\2.png)



# 《小样本学习（Few-shot Learning）综述》

早期的 Few-shot Learning 算法研究多集中在图像领域，如图 2 所示，Few-shot Learning 模型大致可分为三类：Mode Based，Metric Based 和 Optimization Based。

其中 Model Based 方法旨在通过模型结构的设计快速在少量样本上更新参数，直接建立输入 x 和预测值 P 的映射函数；Metric Based 方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类；Optimization Based 方法认为普通的梯度下降方法难以在 few-shot 场景下拟合，因此通过调整优化方法来完成小样本分类的任务。

## Model Based方法

利用循环神经网络的内部记忆单元无法扩展到需要对大量新信息进行编码的新任务上。因此，需要让存储在记忆中的表达既要稳定又要是元素粒度访问的，前者是说当需要时就能可靠地访问，后者是说可选择性地访问相关的信息；另外，参数数量不能被内存的大小束缚。神经图灵机（NTMs）和记忆网络就符合这种必要条件。

## matric based

孪生网络（Siamese Network）[4] 通过有监督的方式训练孪生网络来学习，然后重用网络所提取的特征进行 one/few-shot 学习.

匹配网络（Match Network）[2] 为支撑集和 Batch 集构建不同的编码器，最终分类器的输出是支撑集样本和 query 之间预测值的加权求和。

## Optimization Based方法





# 《Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks》

## 阅读原因

PCNN的开山之作，感觉挺重要的，看看。



# 《Entity-Relation Extraction as Multi-turn Question Answering ACL19》

## 阅读原因

基于*阅读理解*的关系抽取。据说这种方式可以在少数标注数据（每个关系20条标注）下实现较好的结果。

参考链接：https://mp.weixin.qq.com/s/khOjgr5z_CH2-1vDYQ1ylw

论文来自香农科技



# 《Zero-Shot Relation Extraction via Reading Comprehension》

## 阅读原因

就还是为了解决标注数据不足的情况下关系抽取的问题呗。






# 《Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification》

## 阅读原因

提出多级匹配和整合结构，充分学习训练样例之间的潜在关联，尽可能挖掘为数不多的样例中的潜在信息。少次学习的思想。



# 《Matching the Blanks: Distributional Similarity for Relation Learning. (ACL 2019)》

Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, Tom Kwiatkowski. 

## 阅读原因

少次学习。采用了预训练语言模型 BERT 来处理关系抽取中的少次学习问题，基于海量无监督数据训练的 BERT，能够为少次学习模型提供有效的语义特征，在 FewRel 数据上取得了超过人类关系分类的水平。



# 《Cross Attention Network for Few-shot Classification》

该文提出了一个名为交叉注意力网络（Cross Attention Network）的模型，一方面通过注意力机制建立待分类类别特征与查询样本之间的联系，并且突出目标个体所在区域，同时建立一种名为直推式学习（transductive learning）的半监督推理来解决少样本的数据缺乏困境，最终同样是在两个少样本的标准数据集上取得了性能的提升。


# 《Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition》

针对少样本细粒度识别分类（one-shot fine-grained visual recognition）任务下缺乏数据的问题，该文提出了用生成网络合成图像数据，利用元学习的方法将其于真实数据混合，放进名为 MetaIRNet(Meta Image Reinforcing Network) 的网络模型训练，最终达到识别效果的提升。

# 《Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot Learning》

该文针对泛化零样本学习问题（generalized zero-shot learning, GZSL）开创性地提出了一个双重对抗式语义连续网络（Dual Adversarial Semantics-Consistent Network, DASCN），在一个统一的 GZSL 问题框架下，用其学习原生 GAN 与其对偶的 GAN 网络，从而达到更好的任务识别效果。



# 《Unsupervised Meta-Learning for Few-Shot Image Classification》

这篇文章同样是针对少样本的分类学习问题，提出一种无监督式的元学习模型 UMTRA，并在两个数据集上取得了非常优秀的分类效果。


# 《Transductive Zero-Shot Learning with Visual Structure Constraint》

该文提出一般的零样本学习方法都容易在数据分布的原生域（source domain）到目标域（target domain）的映射过程中出现局部偏移（domain shift）导致学习效果不尽如人意。文章借此提出一种新的视觉结构限制（visual structure constrain）来提升映射函数的泛化性，从而避免上述提到的偏移缺点，文章采用了新的训练策略，应用了提出的限制模块，在标准数据集上取得了不错的效果。


# 《Order Optimal One-Shot Distributed Learning》

文章提出了一种名为多分辨率预计（Multi-Resolution Estimator, MRE）的新算法，将其应用于少样本学习过程里的分布式统计优化。


# 《Generative Adversarial Zero-Shot Relation Learning for Knowledge Grapths》

https://zhuanlan.zhihu.com/p/112908641



# 《Learning Dual Retrieval Module for Semi-supervised Relation Extraction》
半监督方法进行关系抽取。与现有工作有一定吻合。





# 《DocRED: A Large-Scale Document-Level Relation Extraction Dataset》(ACL 2019 )

论文地址：https://arxiv.org/abs/1906.06127

GitHub地址：https://github.com/thunlp/DocRED

# 《Matching the Blanks: Distributional Similarity for Relation Learning》

论文地址：https://arxiv.org/abs/1906.03158

Github地址：https://github.com/zhpmatrix/BERTem


26.《GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extractionn》(ACL 2019)

论文地址：https://tsujuifu.github.io/projs/acl19_graph-rel.html

GitHub地址：https://github.com/tsujuifu/pytorch_graph-rel



1. AutoML strategy based on grammatical evolution: A case study about knowledge discovery from text
Suilan Estevez-Velarde, Yoan Gutiérrez, Andrés Montoyo and Yudivián Almeida-Cruz

2. Explicit Utilization of General Knowledge in Machine Reading Comprehension

https://cloud.tencent.com/developer/news/471794
