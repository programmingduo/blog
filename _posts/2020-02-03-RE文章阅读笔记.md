---
layout: post
title: "RE-文章阅读笔记r"
description: ""
categories: []
tags: []
redirect_from:
  - /2020/02/03/
---

* Karmdown table of content
{:toc .toc}

# https://new.qq.com/omn/20191118/20191118A08U8T00.html

FewRel 论文工作初步尝试了几个代表性少次学习方法包括度量学习（Metric learning）、元学习（Meta learning）、参数预测（Parameter prediction）等，评测表明即使是效果最佳的原型网络（Prototypical Networks）模型，在少次关系抽取上的性能仍与人类表现相去甚远

目前研究方向：

1. 更大规模的训练数据：如何提出更有效的机制来高效获取高质量、高覆盖、高平衡的训练数据
2. 更高效的学习能力：探索少次学习关系抽取，让关系抽取模型具备更强大高效的学习能力。
3. 更复杂的文本语境：文档级关系抽取任务要求模型具有强大的模式识别、逻辑推理、指代推理和常识推理能力
4. 更开放的关系类型

在韩旭和高天宇等同学的努力下，发布了 OpenNRE 工具包 [33]，经过近两年来的不断改进，涵盖有监督关系抽取、远程监督关系抽取、少次学习关系抽取和文档级关系抽取等丰富场景。此外，也花费大量科研经费标注了 FewRel（1.0 和 2.0）和 DocRED 等数据集，旨在推动相关方向的研究。


# 《小样本学习的悖论》

作者：Matrix-11
链接：https://zhuanlan.zhihu.com/p/120056051
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

在讨论小样本学习为什么不靠谱之前，我们先来看看目前的小样本学习基本都是一些什么方法吧。
特征迁移
特征迁移，大概是一个放之四海而皆可的一种方法，特征迁移，也有人认为是迁移学习的一种，一个模型先在大量的数据上训练过，具备了一定的特征表达能力，然后在新的数据集上，做一遍特征提取，用少量的样本训练一个线性分类模型，这个分类模型，用来预测，这个看起来就是一个标准的特征提取，训练模型，做预测的流程，只是以前的特征提取是基于手工设计的，比如 HOG, LBP, SIFT 什么的，现在换成了一个网络，这和小样本学习似乎也没有什么关系。这个方法，在很多小样本学习里，其实效果反而不错，比起那些绞尽脑汁想出的古怪方法稳定，高效。
度量匹配
度量学习，也是一个比较通用的一种方法，上面说的特征迁移，是训练网络做特征提取器，那度量学习，就是训练网络做度量，最后尽可能让同类的特征比较靠近，不同的特征隔开，这个网络，最后也是用来做特征提取，不过不再有分类器了，而是直接用一个特征原型来做分类，每个类别，都有一个特征原型，新来的数据，每个类别少量的几个样本，也能构成一个特征原型，一般都是直接求平均，这样做分类的时候，就是测试图片的特征和每个类别的特征原型做比较，这就是度量匹配的基本原理，这个方法和上面的特征迁移类似，也是把网络当做一个特征表达器，寻找类别的原型，还是没有看出和小样本学习有啥关系。
数据增广
这个更奇怪，虽然说是小样本学习，最后提出了一个数据增广的方法来解决小样本学习问题，这类方法的出发点是，既然我能拿到的样本很少，那我就想办法增广样本，好了，既然是增广，那就有各种想法了，最常见的一种是用 GAN，先用大量的数据训练一个 GAN，然后再用 GAN 来生成样本进行扩充，这种方法，我总觉得哪里不对。还有一种是对图片本身的各种操作，或者不同图片之间的各种组合，反正就是各种尝试，看哪种组合方式有效，就用哪种。数据增广这种方式，也是比较有效的一种。
其他
除了上面几种主流方法之外，还有其它一些比较清奇的，比如用 meta learning 的方式来训练网络，然后让网络可以适应小样本的学习方式，还有利用特征融合的，比如利用语义特征，来做小样本学习的。
小样本学习与信号采样
说了这么多，发现这些方法除了数据集换成小样本之外，其他的和常规的机器学习方法似乎没有什么不同，而且所有的小样本学习的方法，在数据量稍微多一点的时候，就和上面最简单的特征迁移的方法效果基本一致，甚至还不如简单的特征迁移方法。