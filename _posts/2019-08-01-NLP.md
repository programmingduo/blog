---
layout: post
title: "NLP"
description: "nlp 相关"
categories: [NLP]
tags: []
redirect_from:
  - /2020/01/17/
---

* Karmdown table of content
{:toc .toc}


# 评测数据集

## CMMLU

CMMLU涵盖了从基础学科到高级专业水平的67个主题。它包括：需要计算和推理的自然科学，需要知识的人文科学和社会科学,以及需要生活常识的中国驾驶规则等。此外，CMMLU中的许多任务具有中国特定的答案，可能在其他地区或语言中并不普遍适用。因此是一个完全中国化的中文测试基准。

链接：https://github.com/haonan-li/CMMLU

数据集都是选择题。回答包括直接给出答案和给出思考链路两种方式。

## MMLU

英文数据集。它考虑了57 个学科，从人文到社科到理工多个大类的综合知识能力。数据集也是选择题。

## CEval 

C-Eval 是一个全面的中文基础模型评估套件。它包含了13948个多项选择题，涵盖了52个不同的学科和四个难度级别：初中、高中、大学和专业。


# 生成类任务的评估指标


# 单词表达

单词表达一般只有两种：One hot representation和Distributed representation

用输入单词作为中心单词去预测周边单词的方式：Word2Vec。http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/

用输入单词作为周边单词去预测中心单词的方式叫做：Continuous Bag of Words (CBOW)。https://iksinc.online/tag/continuous-bag-of-words-cbow/

# embedding VS One-Hot

参考原文：https://zhuanlan.zhihu.com/p/46016518

Embedding 是一个将离散变量转为连续向量表示的一个方式。

我们可以总结一下，embedding 有以下 3 个主要目的：

在 embedding 空间中查找最近邻，这可以很好的用于根据用户的兴趣来进行推荐。
作为监督性学习任务的输入。
用于可视化不同离散变量之间的关系。

One-hot 编码是一种最普通常见的表示离散数据的表示。首先我们计算出需要表示的离散或类别变量的总个数 N，然后对于每个变量，我们就可以用 N-1 个 0 和单个 1 组成的 vector 来表示每个类别。

这样做有两个很明显的缺点：

对于具有非常多类型的类别变量，变换后的向量维数过于巨大，且过于稀疏。
映射之间完全独立，并不能表示出不同类别之间的关系。

One-hot 编码的最大问题在于其转换不依赖于任何的内在关系。而通过一个监督性学习任务的网络，我们可以通过优化网络的参数和权重来减少 loss 以改善我们的 embedding 表示，loss 越小，则表示最终的向量表示中，越相关的类别，它们的表示越相近。

# 常用几种距离的标识

https://blog.csdn.net/g1036583997/article/details/80606789

# part-of-speech(词性标注)

词性标注目前最好的算法正确率已经达到了97%。事实上，即使使用最基础的算法(假设每个单词的标记都是它出现频率最高的标记，不认识的单词就标记为名词)，词性标注的正确率也已经达到了90%。

因此，可以认为，词性标注效果几乎理想。
