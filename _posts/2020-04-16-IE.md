---
layout: post
title: "IE-paper"
description: ""
categories: []
tags: []
redirect_from:
  - /2020/04/16/
---

* Karmdown table of content
{:toc .toc}


# 基于BERT预训练的中文命名实体识别TensorFlow实现

GitHub： https://github.com/macanv/BERT-BiLSTM-CRF-NER

# IE综述

知乎：https://zhuanlan.zhihu.com/p/74803327

## NER

### 常规数据集

训练集、验证集、测试集以“7:1:2”的比例划分。其中训练集达到49600条的样本数，标注实体共88192个；验证集为7000条，包含12420个标注实体；测试集为14000条，标注实体共25780个。

1. 实验表明，两者在相同的迭代次数训练后，测试集的F1值上BertNER比NeuroNER高出超过4个百分点。即使NeuroNER迭代epoch增加到100，仍然是BertNER的识别效果更优。
2. Bert NER在训练时长、模型加载速度、预测速度上都占据了很大的优势，达到工业级的水平，更适合应用在生产环境当中。
3. 综上所述，Bert-BiLSTM-CRF模型在中文命名实体识别的任务中完成度更高。

### 小数据集

1. BertNER在小数据集甚至极小数据集的情况下，测试集F1值均能达到92以上的水平，证明其也能在常见的文本命名实体识别任务中达到同样优秀的效果。
2. 实验结果证明，利用小数据集训练，可以大大降低人工标注成本的同时，训练时长也越少，也将极大地提高模型迭代的能力，有利于更多实体类型的NER模型构建。
3. 经过效能分析可以看出，数据量往上增加的同时，训练时长以相同的比例增加，而F1值提升的幅度在逐渐下降。因此，我们在扩充实体类别的时候，可以参考此效能比例，从而衡量所要投入的资源以及所能达到的模型效果。

## 词性标注的使用

经过NER、分词、词性标注的对比测试后发现，Jieba分词同时具有速度快和支持用户自定义词典的两大优点，Pyltp具有单独使用词性标注的灵活性。因此，使用“Jieba分词 + BertNER作自定义词典 + Pyltp词性标注”的组合策略后，可以弥补Jieba分词在实体识别的缺点，保证较高的准确率和产品速度。

## 指代消解

大部分工具包都是基于语义结构中的词和句的规则来实现指代消解，而且都是在英文的语言结构当中实现了不错的效果，NeuralCoref和AllenNLP不支持中文，而Stanford coreNLP 是具有多种语言模型，其中包括了中文模型，但Stanford coreNLP 的指代消解在中文的表现并不理想。目前而言，基于深度学习的端到端指代消解模型还达不到生产应用的要求。

经过反复的实验表明，*基于BertNER的中文指代消解框架比基于Stanford coreNLP的指代消解模型在中文上获得更高的准确率和更好的效果，同时实现了主语补齐的功能，有助于抽取更多的有用三元组信息。*

# 《A Novel Hierarchical Binary Tagging Framework for Relational Triple Extraction》

相较于feature-based方法，neural network based 方法可以有效解决特征工程中严重依赖人力的问题。并且可以获得很好的效果。

# 《Joint Extraction of Entities and Overlapping Relations Using Position-Attentive Sequence Labeling》

来自百度的一篇论文。F1只有50。。

## introduction

将信息抽取过程转化成了一个序列标注，同时对实体和关系进行序列标注。对于一个长度为n的句子来说，首先对每一个位置产生一个序列，故共有n个标注序列。之后使用一个position-attention mechanism将n个标注序列转换为n个position-aware 的句子表示，然后根据句子表示解码为tagging result，之后进行抽取。

## methodology

### tagging schema

![smiley](\assets\images\usedInBlogs\IE\0.png)

对于当前位置，如果是实体，使用“BIES” (Begin, Inside, End, Single)进行标注，如果另一实体与当前位置实体有某种关系，则使用关系类型进行标注。

## End-to-End Sequence Labeling Model with Position-Attention

1. 使用 RNN encoder 对n-word的句子进行encode
2. 对每一个位置产生的标注序列使用position-attention mechanism产生对应的句子表示
3. 使用Conditional Random Field (CRF)抽取实体和关系
















